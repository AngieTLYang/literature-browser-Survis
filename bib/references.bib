@article{Beck2016Visual,
  abstract = {Bibiographic data such as collections of scientific articles and citation networks have been studied extensively in information visualization and visual analytics research. Powerful systems have been built to support various types of bibliographic analysis, but they require some training and cannot be used to disseminate the insights gained. In contrast, we focused on developing a more accessible visual analytics system, called SurVis, that is ready to disseminate a carefully surveyed literature collection. The authors of a survey may use our Web-based system to structure and analyze their literature database. Later, readers of the survey can obtain an overview, quickly retrieve specific publications, and reproduce or extend the original bibliographic analysis. Our system employs a set of selectors that enable users to filter and browse the literature collection as well as to control interactive visualizations. The versatile selector concept includes selectors for textual search, filtering by keywords and meta-information, selection and clustering of similar publications, and following citation links. Agreement to the selector is represented by word-sized sparkline visualizations seamlessly integrated into the user interface. Based on an analysis of the analytical reasoning process, we derived requirements for the system. We developed the system in a formative way involving other researchers writing literature surveys. A questionnaire study with 14 visual analytics experts confirms that SurVis meets the initially formulated requirements.},
  author = {Beck, Fabian and Koch, Sebastian and Weiskopf, Daniel},
  doi = {10.1109/TVCG.2015.2467757},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  keywords = {type:system, visual_analytics, sparklines, information_retrieval, clustering, literature_browser},
  number = {01},
  publisher = {IEEE},
  volume = {22},
  series = {TVCG},
  title = {Visual Analysis and Dissemination of Scientific Literature Collections with {SurVis}},
  url = {http://www.visus.uni-stuttgart.de/uploads/tx_vispublications/vast15-survis.pdf},
  year = {2016}
}
@article{liu2024ocrbench,
  abstract = {Large models have recently played a dominant role in natural language processing and multimodal vision-language learning. However, their effectiveness in text-related visual tasks remains relatively unexplored. In this paper, we conducted a comprehensive evaluation of large multimodal models, such as GPT4V and Gemini, in various text-related visual tasks including text recognition, scene text-centric visual question answering (VQA), document-oriented VQA, key information extraction (KIE), and handwritten mathematical expression recognition (HMER). To facilitate the assessment of optical character recognition (OCR) capabilities in large multimodal models, we propose OCRBench, a comprehensive evaluation benchmark. OCRBench contains 29 datasets, making it the most comprehensive OCR evaluation benchmark available. Furthermore, our study reveals both the strengths and weaknesses of these models, particularly in handling multilingual text, handwritten text, non-semantic text, and mathematical expression recognition. Most importantly, the baseline results presented in this study could provide a foundational framework for the conception and assessment of innovative strategies targeted at enhancing zero-shot multimodal techniques. The evaluation pipeline and benchmark are available at https://github.com/Yuliang-Liu/MultimodalOCR.},
  author = {Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin & Xiang Bai},
  doi = {https://doi.org/10.1007/s11432-024-4235-6},
  journal = {Science China Information Sciences},
  keywords = {type: benchmark, ocr, large_models, multimodal_learning, vision_language, vqa, kie, handwritten_text, mathematical_recognition},
  number = {220102},
  publisher = {Science China Press},
  volume = {67},
  series = {},
  title = {OCRBench: on the hidden mystery of OCR in large multimodal models},
  url = {https://link.springer.com/article/10.1007/s11432-024-4235-6#Abs1},
  year = {2024}
}
@article{li2023trocr,
  abstract = {Text recognition is a long-standing research problem for document digitalization. Existing approaches are usually built based on CNN for image understanding and RNN for char-level text generation. In addition, another language model is usually needed to improve the overall accuracy as a post-processing step. In this paper, we propose an end-to-end text recognition approach with pre-trained image Transformer and text Transformer models, namely TrOCR, which leverages the Transformer architecture for both image understanding and word piece-level text generation. The TrOCR model is simple but effective, and can be pre-trained with large-scale synthetic data and fine-tuned with human-labeled datasets. Experiments show that the TrOCR model outperforms the current state-of-the-art models on the printed, handwritten and scene text recognition tasks. The TrOCR models and code are publicly available.},
  author = {Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu WeiAuthors Info & Claims},
  doi = {https://doi.org/10.1609/aaai.v37i11.26538},
  journal = {The Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)},
  keywords = {type: SNLP: Applications, CV: Language and Vision},
  number = {11},
  publisher = {AAAI Press},
  volume = {37},
  series = {},
  title = {TrOCR: transformer-based optical character recognition with pre-trained models},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/26538},
  year = {2023}
}
@article{kojima2022zeroshot,
  abstract = {Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding "Let's think step by step" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large-scale InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.},
  author = {Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, Yusuke Iwasawa},
  doi = {10.48550/arXiv.2205.11916},
  journal = {NIPS '22: Proceedings of the 36th International Conference on Neural Information Processing Systems},
  keywords = {type: large language models, zero-shot learning, chain-of-thought prompting, reasoning, natural language processing, arithmetic reasoning, symbolic reasoning, prompt engineering, in-context learning, multi-task learning},
  number = {1613},
  publisher = {Curran Associates, Inc.},
  volume = {},
  series = {},
  title = {Large language models are zero-shot reasoners},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf},
  year = {2022}
}
@article{masry2022chartqa,
  abstract = {Charts are very popular for analyzing data. When exploring charts, people often ask a variety of complex reasoning questions that involve several logical and arithmetic operations. They also commonly refer to visual features of a chart in their questions. However, most existing datasets do not focus on such complex reasoning questions as their questions are template-based and answers come from a fixed-vocabulary. In this work, we present a large-scale benchmark covering 9.6K human-written questions as well as 23.1K questions generated from human-written chart summaries. To address the unique challenges in our benchmark involving visual and logical reasoning over charts, we present two transformer-based models that combine visual features and the data table of the chart in a unified way to answer questions. While our models achieve the state-of-the-art results on the previous datasets as well as on our benchmark, the evaluation also reveals several challenges in answering complex reasoning questions.},
  author = {Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, Enamul Hoque},
  doi = {10.18653/v1/2022.findings-acl.177},
  journal = {ACL 2022},
  keywords = {type: chart question answering, visual reasoning, logical reasoning, multimodal learning, data visualization understanding, benchmark dataset, information extraction, machine comprehension, visual question answering, chart understanding},
  number = {},
  publisher = {Association for Computational Linguistics},
  volume = {Findings of the Association for Computational Linguistics: ACL 2022},
  series = {},
  title = {ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning},
  url = {https://aclanthology.org/2022.findings-acl.177/},
  year = {2022}
}
@article{mathew2021docvqa, 
  abstract = {We present a new dataset for Visual Question Answering(VQA) on document images called DocVQA. The dataset consists of 50,000 questions defined on 12,000+ document images. Detailed analysis of the dataset in comparison with similar datasets for VQA and reading comprehension is presented. We report several baseline results by adopting existing VQA and reading comprehension models. Although the existing models perform reasonably well on certain types of questions, there is large performance gap compared to human performance (94.36% accuracy). The models need to improve specifically on questions where understanding structure of the document is crucial. The dataset, code and leaderboard are available at docvqa.org},
  author = {Minesh Mathew, Dimosthenis Karatzas and C. V. Jawahar},
  doi = {10.1109/WACV48630.2021.00225},
  journal = {2021 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  keywords = {type: dataset, vqa, document_understanding},
  number = {},
  publisher = {IEEE},
  volume = {},
  series = {},
  title = {DocVQA: A Dataset for VQA on Document Images},
  url = {https://www.computer.org/csdl/proceedings-article/wacv/2021/047700c199/1uqGfrEmYVi},
  year = {2021}  
}
@article{Kim2025_VQA_Survey,
  abstract = {Visual question answering (VQA) is a dynamic field of research that aims to generate textual answers from given visual and question information. It is a multimodal field that has garnered significant interest from the computer vision and natural language processing communities. Furthermore, recent advances in these fields have yielded numerous achievements in VQA research. In VQA research, achieving balanced learning that avoids bias toward either visual or question information is crucial. The primary challenge in VQA lies in eliminating noise, while utilizing valuable and accurate information from different modalities. Various research methodologies have been developed to address these issues. In this study, we classify these research methods into three categories: Joint Embedding, Attention Mechanism, and Model-agnostic methods. We analyze the advantages, disadvantages, and limitations of each approach. In addition, we trace the evolution of datasets in VQA research, categorizing them into three types: Real Image, Synthetic Image, and Unbiased datasets. This study also provides an overview of evaluation metrics based on future research directions. Finally, we discuss future research and application directions for VQA research. We anticipate that this survey will offer useful perspectives and essential information to researchers and practitioners seeking to address visual questions effectively.},
  author = {Byeong Su Kim, Jieun Kim, Deokwoo Lee, Beakcheol Jang},
  doi = {https://doi.org/10.1145/372863},
  journal = {ACM Computing Surveys},
  keywords = {type: Visual Question Answering, Multimodal Learning, Attention Mechanism, Joint Embedding, VQA Datasets, Evaluation Metrics, Computer Vision, NLP},
  number = {},
  publisher = {Association for Computing Machinery},
  volume = {57},
  series = {},
  title = {Visual Question Answering: A Survey of Methods, Datasets, Evaluation, and Challenges},
  url = {https://dl.acm.org/doi/10.1145/3728635},
  year = {2025}
}
@article{Long2021_SceneText_DL,
  abstract = {With the rise and development of deep learning, computer vision has been tremendously transformed and reshaped. As an important research area in computer vision, scene text detection and recognition has been inevitably influenced by this wave of revolution, consequentially entering the era of deep learning. In recent years, the community has witnessed substantial advancements in mindset, methodology and performance. This survey is aimed at summarizing and analyzing the major changes and significant progresses of scene text detection and recognition in the deep learning era. Through this article, we devote to: (1) introduce new insights and ideas; (2) highlight recent techniques and benchmarks; (3) look ahead into future trends. Specifically, we will emphasize the dramatic differences brought by deep learning and remaining grand challenges. We expect that this review paper would serve as a reference book for researchers in this field. Related resources are also collected in our Github repository (https://github.com/Jyouhou/SceneTextPapers).},
  author = {Shangbang Long, Xin He, Cong Yao},
  doi = {https://doi.org/10.1007/s11263-020-01369-},
  journal = {International Journal of Computer Vision},
  keywords = {type: scene text detection, scene text recognition, deep learning, optical character recognition, text spotting, computer vision, convolutional neural networks, sequence modeling, text extraction, end-to-end text recognition},
  number = {},
  publisher = {Association for Computing Machinery},
  volume = {129},
  series = {},
  title = {Scene Text Detection and Recognition: The Deep Learning Era},
  url = {https://dl.acm.org/doi/10.1007/s11263-020-01369-0},
  year = {2021}
}
@article{Migicovsky2014_SmartwatchSecurity,
  abstract = {Many companies have recently started to offer wearable computing devices including glasses, bracelets, and watches. While this technology enables exciting new applications, it also poses new security and privacy concerns. In this work, we explore these implications and analyze the impact of one of the first networked wearable devices—smartwatches— on an academic environment. As a proof of concept, we develop an application for the Pebble smartwatch called ConTest that would allow dishonest students to inconspicuously collaborate on multiple-choice exams in real time, using a cloud-based service, a smartphone, and a client application on a smartwatch. We discuss the broader implications of this technology, suggest hardware and software approaches that can be used to prevent such attacks, and pose questions for future research.},
  author = {Alex Migicovsky, Zakir Durumeric, Jeff Ringenberg, and J. Alex Halderman},
  doi = {https://doi.org/10.1007/978-3-662-45472-5_7},
  journal = {18th International Conference Financial Cryptography and Data Security},
  keywords = {type: security, wearable computing, smartwatches, cheating},
  number = {},
  publisher = {Springer Nature},
  volume = {},
  series = {},
  title = {Outsmarting Proctors with Smartwatches: A Case Study on Wearable Computing Security},
  url = {https://jhalderm.com/pub/papers/smartwatch13.pdf},
  year = {2014}
}
@article{Lee2018_SmartGlassesSurvey,
  abstract = {Since the launch of Google Glass in 2014, smart glasses have mainly been designed to support micro-interactions. The ultimate goal for them to become an augmented reality interface has not yet been attained due to an encumbrance of controls. Augmented reality involves superimposing interactive computer graphics images onto physical objects in the real world. This survey reviews current research issues in the area of human-computer interaction for smart glasses. The survey first studies the smart glasses available in the market and afterwards investigates the interaction methods proposed in the wide body of literature. The interaction methods can be classified into hand-held, touch, and touchless input. This paper mainly focuses on the touch and touchless input. Touch input can be further divided into on-device and on-body, while touchless input can be classified into hands-free and freehand. Next, we summarize the existing research efforts and trends, in which touch and touchless input are evaluated by a total of eight interaction goals. Finally, we discuss several key design challenges and the possibility of multi-modal input for smart glasses.},
  author = {LIK-HANG LEE, AND PAN HUI, (Fellow, IEEE)},
  doi = {10.1109/ACCESS.2018.2831081},
  journal = {IEEE Access},
  keywords = {type: Input methods, smart glasses interaction, touch inputs, touchless input, wearable computing.},
  number = {},
  publisher = {IEEE},
  volume = {6},
  series = {},
  title = {Interaction Methods for Smart Glasses: A Survey},
  url = {https://core.ac.uk/download/224633694.pdf},
  year = {2018}
}
@article{urbanski2025lvms,
  abstract = {This article assesses the threat of LVLMs and smart glasses that interface with them towards the integrity of in-person exams proposes and utilizes a new specifically developed benchmark based on standardized exam questions. While performance decreases under image degradation are being demonstrated, it still highlights the high accuracy of publicly available models when answering exam questions, even under less-than-optimal conditions, which showcases the need for researching more robust exams. Additionally, approaches to developing benchmarks whose performance translates better to real-life scenarios are being demonstrated, along with quantifying expected performance detriments when moving from synthetic benchmarks under ideal conditions to similar practical applications.},
  author = {Rupert Urbanski, Ralf Peters},
  doi = {10.5220/0013206100003932},
  journal = {In Proceedings of the 17th International Conference on Computer Supported Education (CSEDU 2025)},
  keywords = {type: Large Language Models, Large Vision-Language Models, LLM, LVLM, Exam, Threat, Integrity, Smart Glasses, Computer Vision, CV, Cheating, Smart Devices, Benchmark. },
  number = {},
  publisher = {SciTePress - SCIENCE AND TECHNOLOGY PUBLICATIONS},
  volume = {2},
  series = {},
  title = {L(V)LMs Compromising the Integrity of in-Person Exams: An Evaluation Utilizing Smart Glasses and Computer Vision},
  url = {https://www.scitepress.org/Papers/2025/132061/132061.pdf},
  year = {2025}
}
